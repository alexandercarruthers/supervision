{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"\ud83d\udc4b Welcome","text":"<p>Supervision is a set of easy-to-use utilities that will come in handy in any computer vision project. </p> <p>Supervision is still in  pre-release stage \ud83d\udea7 Keep your eyes open for potential bugs and be aware that at this stage our API is still fluid and may change.</p>"},{"location":"#how-to-install","title":"\ud83d\udcbb How to Install","text":"<p>You can install <code>supervision</code> with pip in a  3.10&gt;=Python&gt;=3.7 environment.</p> <p>Pip install method (recommended)</p> <pre><code>pip install supervision\n</code></pre> <p>Git clone method (for development)</p> <p><pre><code>git https://github.com/roboflow/supervision.git\ncd supervision\npip install -e '.[dev]'\n</code></pre> See contributing section to know more about contributing to the project</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#052-april-13-2023","title":"0.5.2 April 13, 2023","text":"<ul> <li>Fixed [#63]: <code>LineZone.trigger</code> function expects 4 values instead of 5.</li> </ul>"},{"location":"changelog/#051-april-12-2023","title":"0.5.1 April 12, 2023","text":"<ul> <li>Fixed <code>Detections.__getitem__</code> method did not return mask for selected item.</li> <li>Fixed <code>Detections.area</code> crashed for mask detections.</li> </ul>"},{"location":"changelog/#050-april-10-2023","title":"0.5.0 April 10, 2023","text":"<ul> <li>Added [#58]: <code>Detections.mask</code> to enable segmentation support.</li> <li>Added [#58]: <code>MaskAnnotator</code> to allow easy <code>Detections.mask</code> annotation.</li> <li>Added [#58]: <code>Detections.from_sam</code> to enable native Segment Anything Model (SAM) support.</li> <li>Changed [#58]: <code>Detections.area</code> behaviour to work not only with boxes but also with masks.</li> </ul>"},{"location":"changelog/#040-april-5-2023","title":"0.4.0 April 5, 2023","text":"<ul> <li>Added [#46]: <code>Detections.empty</code> to allow easy creation of empty <code>Detections</code> objects.</li> <li>Added [#56]: <code>Detections.from_roboflow</code> to allow easy creation of <code>Detections</code> objects from Roboflow API inference results.</li> <li>Added [#56]: <code>plot_images_grid</code> to allow easy plotting of multiple images on single plot.</li> <li>Added [#56]: initial support for Pascal VOC XML format with <code>detections_to_voc_xml</code> method.</li> <li>Changed [#56]: <code>show_frame_in_notebook</code> refactored and renamed to <code>plot_image</code>.</li> </ul>"},{"location":"changelog/#032-march-23-2023","title":"0.3.2 March 23, 2023","text":"<ul> <li>Changed [#50]: Allow <code>Detections.class_id</code> to be <code>None</code>. </li> </ul>"},{"location":"changelog/#031-march-6-2023","title":"0.3.1 March 6, 2023","text":"<ul> <li>Fixed [#41]: <code>PolygonZone</code> throws an exception when the object touches the bottom edge of the image.</li> <li>Fixed [#42]: <code>Detections.wth_nms</code> method throws an exception when <code>Detections</code> is empty.</li> <li>Changed [#36]: <code>Detections.wth_nms</code> support class agnostic and non-class agnostic case.</li> </ul>"},{"location":"changelog/#030-march-6-2023","title":"0.3.0 March 6, 2023","text":"<ul> <li>Changed: Allow <code>Detections.confidence</code> to be <code>None</code>.</li> <li>Added: <code>Detections.from_transformers</code> and <code>Detections.from_detectron2</code> to enable seamless integration with Transformers and Detectron2 models. </li> <li>Added: <code>Detections.area</code> to dynamically calculate bounding box area.</li> <li>Added: <code>Detections.wth_nms</code> to filter out double detections with NMS. Initial - only class agnostic - implementation. </li> </ul>"},{"location":"changelog/#020-february-2-2023","title":"0.2.0 February 2, 2023","text":"<ul> <li>Added: Advanced <code>Detections</code> filtering with pandas-like API.</li> <li>Added: <code>Detections.from_yolov5</code> and <code>Detections.from_yolov8</code> to enable seamless integration with YOLOv5 and YOLOv8 models.</li> </ul>"},{"location":"changelog/#010-january-19-2023","title":"0.1.0 January 19, 2023","text":"<p>Say hello to Supervision \ud83d\udc4b</p>"},{"location":"video/","title":"Video","text":""},{"location":"video/#videoinfo","title":"VideoInfo","text":"<p>A class to store video information, including width, height, fps and total number of frames.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>width of the video in pixels</p> <code>height</code> <code>int</code> <p>height of the video in pixels</p> <code>fps</code> <code>int</code> <p>frames per second of the video</p> <code>total_frames</code> <code>int</code> <p>total number of frames in the video, default is None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from supervision import VideoInfo\n\n&gt;&gt;&gt; video_info = VideoInfo.from_video_path(video_path='video.mp4')\n\n&gt;&gt;&gt; video_info\nVideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\n&gt;&gt;&gt; video_info.resolution_wh\n(3840, 2160)\n</code></pre> Source code in <code>supervision/video.py</code> <pre><code>@dataclass\nclass VideoInfo:\n\"\"\"\n    A class to store video information, including width, height, fps and total number of frames.\n\n    Attributes:\n        width (int): width of the video in pixels\n        height (int): height of the video in pixels\n        fps (int): frames per second of the video\n        total_frames (int, optional): total number of frames in the video, default is None\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; from supervision import VideoInfo\n\n        &gt;&gt;&gt; video_info = VideoInfo.from_video_path(video_path='video.mp4')\n\n        &gt;&gt;&gt; video_info\n        VideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\n        &gt;&gt;&gt; video_info.resolution_wh\n        (3840, 2160)\n        ```\n    \"\"\"\n\n    width: int\n    height: int\n    fps: int\n    total_frames: Optional[int] = None\n\n    @classmethod\n    def from_video_path(cls, video_path: str) -&gt; VideoInfo:\n        video = cv2.VideoCapture(video_path)\n        if not video.isOpened():\n            raise Exception(f\"Could not open video at {video_path}\")\n\n        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(video.get(cv2.CAP_PROP_FPS))\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n        video.release()\n        return VideoInfo(width, height, fps, total_frames)\n\n    @property\n    def resolution_wh(self) -&gt; Tuple[int, int]:\n        return self.width, self.height\n</code></pre>"},{"location":"video/#videosink","title":"VideoSink","text":"<p>Context manager that saves video frames to a file using OpenCV.</p> <p>Attributes:</p> Name Type Description <code>target_path</code> <code>str</code> <p>The path to the output file where the video will be saved.</p> <code>video_info</code> <code>VideoInfo</code> <p>Information about the video resolution, fps, and total frame count.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from supervision import VideoInfo, VideoSink\n\n&gt;&gt;&gt; video_info = VideoInfo.from_video_path(video_path='source_video.mp4')\n\n&gt;&gt;&gt; with VideoSink(target_path='target_video.mp4', video_info=video_info) as s:\n...     frame = ...\n...     s.write_frame(frame=frame)\n</code></pre> Source code in <code>supervision/video.py</code> <pre><code>class VideoSink:\n\"\"\"\n    Context manager that saves video frames to a file using OpenCV.\n\n    Attributes:\n        target_path (str): The path to the output file where the video will be saved.\n        video_info (VideoInfo): Information about the video resolution, fps, and total frame count.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; from supervision import VideoInfo, VideoSink\n\n        &gt;&gt;&gt; video_info = VideoInfo.from_video_path(video_path='source_video.mp4')\n\n        &gt;&gt;&gt; with VideoSink(target_path='target_video.mp4', video_info=video_info) as s:\n        ...     frame = ...\n        ...     s.write_frame(frame=frame)\n        ```\n    \"\"\"\n\n    def __init__(self, target_path: str, video_info: VideoInfo):\n        self.target_path = target_path\n        self.video_info = video_info\n        self.__fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        self.__writer = None\n\n    def __enter__(self):\n        self.__writer = cv2.VideoWriter(\n            self.target_path,\n            self.__fourcc,\n            self.video_info.fps,\n            self.video_info.resolution_wh,\n        )\n        return self\n\n    def write_frame(self, frame: np.ndarray):\n        self.__writer.write(frame)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.__writer.release()\n</code></pre>"},{"location":"video/#get_video_frames_generator","title":"get_video_frames_generator","text":"<p>Get a generator that yields the frames of the video.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path of the video file.</p> required <p>Returns:</p> Type Description <code>Generator[np.ndarray, None, None]</code> <p>A generator that yields the frames of the video.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from supervision import get_video_frames_generator\n\n&gt;&gt;&gt; for frame in get_video_frames_generator(source_path='source_video.mp4'):\n...     ...\n</code></pre> Source code in <code>supervision/video.py</code> <pre><code>def get_video_frames_generator(source_path: str) -&gt; Generator[np.ndarray, None, None]:\n\"\"\"\n    Get a generator that yields the frames of the video.\n\n    Args:\n        source_path (str): The path of the video file.\n\n    Returns:\n        (Generator[np.ndarray, None, None]): A generator that yields the frames of the video.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; from supervision import get_video_frames_generator\n\n        &gt;&gt;&gt; for frame in get_video_frames_generator(source_path='source_video.mp4'):\n        ...     ...\n        ```\n    \"\"\"\n    video = cv2.VideoCapture(source_path)\n    if not video.isOpened():\n        raise Exception(f\"Could not open video at {source_path}\")\n    success, frame = video.read()\n    while success:\n        yield frame\n        success, frame = video.read()\n    video.release()\n</code></pre>"},{"location":"video/#process_video","title":"process_video","text":"<p>Process a video file by applying a callback function on each frame and saving the result to a target video file.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path to the source video file.</p> required <code>target_path</code> <code>str</code> <p>The path to the target video file.</p> required <code>callback</code> <code>Callable[[np.ndarray, int], np.ndarray]</code> <p>A function that takes in a numpy ndarray representation of a video frame and an int index of the frame and returns a processed numpy ndarray representation of the frame.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from supervision import process_video\n\n&gt;&gt;&gt; def process_frame(scene: np.ndarray) -&gt; np.ndarray:\n...     ...\n\n&gt;&gt;&gt; process_video(\n...     source_path='source_video.mp4',\n...     target_path='target_video.mp4',\n...     callback=process_frame\n... )\n</code></pre> Source code in <code>supervision/video.py</code> <pre><code>def process_video(\n    source_path: str,\n    target_path: str,\n    callback: Callable[[np.ndarray, int], np.ndarray],\n) -&gt; None:\n\"\"\"\n    Process a video file by applying a callback function on each frame and saving the result to a target video file.\n\n    Args:\n        source_path (str): The path to the source video file.\n        target_path (str): The path to the target video file.\n        callback (Callable[[np.ndarray, int], np.ndarray]): A function that takes in a numpy ndarray representation of a video frame and an int index of the frame and returns a processed numpy ndarray representation of the frame.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; from supervision import process_video\n\n        &gt;&gt;&gt; def process_frame(scene: np.ndarray) -&gt; np.ndarray:\n        ...     ...\n\n        &gt;&gt;&gt; process_video(\n        ...     source_path='source_video.mp4',\n        ...     target_path='target_video.mp4',\n        ...     callback=process_frame\n        ... )\n        ```\n    \"\"\"\n    source_video_info = VideoInfo.from_video_path(video_path=source_path)\n    with VideoSink(target_path=target_path, video_info=source_video_info) as sink:\n        for index, frame in enumerate(\n            get_video_frames_generator(source_path=source_path)\n        ):\n            result_frame = callback(frame, index)\n            sink.write_frame(frame=result_frame)\n</code></pre>"},{"location":"annotation/voc/","title":"Pascal VOC XML","text":""},{"location":"annotation/voc/#detections_to_voc_xml","title":"detections_to_voc_xml","text":"<p>Converts Detections object to Pascal VOC XML format.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>A Detections object containing bounding boxes, class ids, and other relevant information.</p> required <code>classes</code> <code>List[str]</code> <p>A list of class names corresponding to the class ids in the Detections object.</p> required <code>filename</code> <code>str</code> <p>The name of the image file associated with the detections.</p> required <code>width</code> <code>int</code> <p>The width of the image in pixels.</p> required <code>height</code> <code>int</code> <p>The height of the image in pixels.</p> required <code>depth</code> <code>int</code> <p>The number of color channels in the image. Defaults to 3 for RGB images.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An XML string in Pascal VOC format representing the detections.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; xyxy = np.array([\n...     [50, 30, 200, 180],\n...     [20, 40, 150, 190]\n... ])\n&gt;&gt;&gt; class_id = np.array([1, 0])\n&gt;&gt;&gt; detections = Detections(xyxy=xyxy, class_id=class_id)\n\n&gt;&gt;&gt; classes = [\"dog\", \"cat\"]\n\n&gt;&gt;&gt; voc_xml = detections_to_voc_xml(\n...     detections=detections,\n...     classes=classes,\n...     filename=\"image1.jpg\",\n...     width=500,\n...     height=400\n... )\n</code></pre> Source code in <code>supervision/annotation/voc.py</code> <pre><code>def detections_to_voc_xml(\n    detections: Detections,\n    classes: List[str],\n    filename: str,\n    width: int,\n    height: int,\n    depth: int = 3,\n) -&gt; str:\n\"\"\"\n    Converts Detections object to Pascal VOC XML format.\n\n    Args:\n        detections (Detections): A Detections object containing bounding boxes, class ids, and other relevant information.\n        classes (List[str]): A list of class names corresponding to the class ids in the Detections object.\n        filename (str): The name of the image file associated with the detections.\n        width (int): The width of the image in pixels.\n        height (int): The height of the image in pixels.\n        depth (int, optional): The number of color channels in the image. Defaults to 3 for RGB images.\n\n    Returns:\n        str: An XML string in Pascal VOC format representing the detections.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; xyxy = np.array([\n        ...     [50, 30, 200, 180],\n        ...     [20, 40, 150, 190]\n        ... ])\n        &gt;&gt;&gt; class_id = np.array([1, 0])\n        &gt;&gt;&gt; detections = Detections(xyxy=xyxy, class_id=class_id)\n\n        &gt;&gt;&gt; classes = [\"dog\", \"cat\"]\n\n        &gt;&gt;&gt; voc_xml = detections_to_voc_xml(\n        ...     detections=detections,\n        ...     classes=classes,\n        ...     filename=\"image1.jpg\",\n        ...     width=500,\n        ...     height=400\n        ... )\n        ```\n    \"\"\"\n\n    # Create root element\n    annotation = Element(\"annotation\")\n\n    # Add folder element\n    folder = SubElement(annotation, \"folder\")\n    folder.text = \"VOC\"\n\n    # Add filename element\n    fname = SubElement(annotation, \"filename\")\n    fname.text = filename\n\n    # Add source element\n    source = SubElement(annotation, \"source\")\n    database = SubElement(source, \"database\")\n    database.text = \"roboflow.ai\"\n\n    # Add size element\n    size = SubElement(annotation, \"size\")\n    w = SubElement(size, \"width\")\n    w.text = str(width)\n    h = SubElement(size, \"height\")\n    h.text = str(height)\n    d = SubElement(size, \"depth\")\n    d.text = str(depth)\n\n    # Add segmented element\n    segmented = SubElement(annotation, \"segmented\")\n    segmented.text = \"0\"\n\n    # Add object elements\n    for i in range(detections.xyxy.shape[0]):\n        obj = SubElement(annotation, \"object\")\n\n        class_id = detections.class_id[i] if detections.class_id is not None else None\n        name = SubElement(obj, \"name\")\n        name.text = classes[class_id] if class_id is not None else \"unknown\"\n\n        bndbox = SubElement(obj, \"bndbox\")\n        xmin = SubElement(bndbox, \"xmin\")\n        xmin.text = str(int(detections.xyxy[i, 0]))\n        ymin = SubElement(bndbox, \"ymin\")\n        ymin.text = str(int(detections.xyxy[i, 1]))\n        xmax = SubElement(bndbox, \"xmax\")\n        xmax.text = str(int(detections.xyxy[i, 2]))\n        ymax = SubElement(bndbox, \"ymax\")\n        ymax.text = str(int(detections.xyxy[i, 3]))\n\n    # Generate XML string\n    xml_string = parseString(tostring(annotation)).toprettyxml(indent=\"  \")\n\n    return xml_string\n</code></pre>"},{"location":"detection/annotate/","title":"Annotate","text":""},{"location":"detection/annotate/#boxannotator","title":"BoxAnnotator","text":"<p>A class for drawing bounding boxes on an image using detections provided.</p> <p>Attributes:</p> Name Type Description <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color to draw the bounding box, can be a single color or a color palette</p> <code>thickness</code> <code>int</code> <p>The thickness of the bounding box lines, default is 2</p> <code>text_color</code> <code>Color</code> <p>The color of the text on the bounding box, default is white</p> <code>text_scale</code> <code>float</code> <p>The scale of the text on the bounding box, default is 0.5</p> <code>text_thickness</code> <code>int</code> <p>The thickness of the text on the bounding box, default is 1</p> <code>text_padding</code> <code>int</code> <p>The padding around the text on the bounding box, default is 5</p> Source code in <code>supervision/detection/annotate.py</code> <pre><code>class BoxAnnotator:\n\"\"\"\n    A class for drawing bounding boxes on an image using detections provided.\n\n    Attributes:\n        color (Union[Color, ColorPalette]): The color to draw the bounding box, can be a single color or a color palette\n        thickness (int): The thickness of the bounding box lines, default is 2\n        text_color (Color): The color of the text on the bounding box, default is white\n        text_scale (float): The scale of the text on the bounding box, default is 0.5\n        text_thickness (int): The thickness of the text on the bounding box, default is 1\n        text_padding (int): The padding around the text on the bounding box, default is 5\n\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 2,\n        text_color: Color = Color.black(),\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n    ):\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.text_color: Color = text_color\n        self.text_scale: float = text_scale\n        self.text_thickness: int = text_thickness\n        self.text_padding: int = text_padding\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        labels: Optional[List[str]] = None,\n        skip_label: bool = False,\n    ) -&gt; np.ndarray:\n\"\"\"\n        Draws bounding boxes on the frame using the detections provided.\n\n        Parameters:\n            scene (np.ndarray): The image on which the bounding boxes will be drawn\n            detections (Detections): The detections for which the bounding boxes will be drawn\n            labels (Optional[List[str]]): An optional list of labels corresponding to each detection. If labels is provided, the confidence score of the detection will be replaced with the label.\n            skip_label (bool): Is set to True, skips bounding box label annotation.\n        Returns:\n            np.ndarray: The image with the bounding boxes drawn on it\n        \"\"\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        for i in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[i].astype(int)\n            class_id = (\n                detections.class_id[i] if detections.class_id is not None else None\n            )\n            idx = class_id if class_id is not None else i\n            color = (\n                self.color.by_idx(idx)\n                if isinstance(self.color, ColorPalette)\n                else self.color\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n            if skip_label:\n                continue\n\n            text = (\n                f\"{class_id}\"\n                if (labels is None or len(detections) != len(labels))\n                else labels[i]\n            )\n\n            text_width, text_height = cv2.getTextSize(\n                text=text,\n                fontFace=font,\n                fontScale=self.text_scale,\n                thickness=self.text_thickness,\n            )[0]\n\n            text_x = x1 + self.text_padding\n            text_y = y1 - self.text_padding\n\n            text_background_x1 = x1\n            text_background_y1 = y1 - 2 * self.text_padding - text_height\n\n            text_background_x2 = x1 + 2 * self.text_padding + text_width\n            text_background_y2 = y1\n\n            cv2.rectangle(\n                img=scene,\n                pt1=(text_background_x1, text_background_y1),\n                pt2=(text_background_x2, text_background_y2),\n                color=color.as_bgr(),\n                thickness=cv2.FILLED,\n            )\n            cv2.putText(\n                img=scene,\n                text=text,\n                org=(text_x, text_y),\n                fontFace=font,\n                fontScale=self.text_scale,\n                color=self.text_color.as_rgb(),\n                thickness=self.text_thickness,\n                lineType=cv2.LINE_AA,\n            )\n        return scene\n</code></pre>"},{"location":"detection/annotate/#supervision.detection.annotate.BoxAnnotator.annotate","title":"<code>annotate(scene, detections, labels=None, skip_label=False)</code>","text":"<p>Draws bounding boxes on the frame using the detections provided.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The image on which the bounding boxes will be drawn</p> required <code>detections</code> <code>Detections</code> <p>The detections for which the bounding boxes will be drawn</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>An optional list of labels corresponding to each detection. If labels is provided, the confidence score of the detection will be replaced with the label.</p> <code>None</code> <code>skip_label</code> <code>bool</code> <p>Is set to True, skips bounding box label annotation.</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The image with the bounding boxes drawn on it</p> Source code in <code>supervision/detection/annotate.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    labels: Optional[List[str]] = None,\n    skip_label: bool = False,\n) -&gt; np.ndarray:\n\"\"\"\n    Draws bounding boxes on the frame using the detections provided.\n\n    Parameters:\n        scene (np.ndarray): The image on which the bounding boxes will be drawn\n        detections (Detections): The detections for which the bounding boxes will be drawn\n        labels (Optional[List[str]]): An optional list of labels corresponding to each detection. If labels is provided, the confidence score of the detection will be replaced with the label.\n        skip_label (bool): Is set to True, skips bounding box label annotation.\n    Returns:\n        np.ndarray: The image with the bounding boxes drawn on it\n    \"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    for i in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[i].astype(int)\n        class_id = (\n            detections.class_id[i] if detections.class_id is not None else None\n        )\n        idx = class_id if class_id is not None else i\n        color = (\n            self.color.by_idx(idx)\n            if isinstance(self.color, ColorPalette)\n            else self.color\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=self.thickness,\n        )\n        if skip_label:\n            continue\n\n        text = (\n            f\"{class_id}\"\n            if (labels is None or len(detections) != len(labels))\n            else labels[i]\n        )\n\n        text_width, text_height = cv2.getTextSize(\n            text=text,\n            fontFace=font,\n            fontScale=self.text_scale,\n            thickness=self.text_thickness,\n        )[0]\n\n        text_x = x1 + self.text_padding\n        text_y = y1 - self.text_padding\n\n        text_background_x1 = x1\n        text_background_y1 = y1 - 2 * self.text_padding - text_height\n\n        text_background_x2 = x1 + 2 * self.text_padding + text_width\n        text_background_y2 = y1\n\n        cv2.rectangle(\n            img=scene,\n            pt1=(text_background_x1, text_background_y1),\n            pt2=(text_background_x2, text_background_y2),\n            color=color.as_bgr(),\n            thickness=cv2.FILLED,\n        )\n        cv2.putText(\n            img=scene,\n            text=text,\n            org=(text_x, text_y),\n            fontFace=font,\n            fontScale=self.text_scale,\n            color=self.text_color.as_rgb(),\n            thickness=self.text_thickness,\n            lineType=cv2.LINE_AA,\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotate/#maskannotator","title":"MaskAnnotator","text":"<p>A class for overlaying masks on an image using detections provided.</p> <p>Attributes:</p> Name Type Description <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color to fill the mask, can be a single color or a color palette</p> Source code in <code>supervision/detection/annotate.py</code> <pre><code>class MaskAnnotator:\n\"\"\"\n    A class for overlaying masks on an image using detections provided.\n\n    Attributes:\n        color (Union[Color, ColorPalette]): The color to fill the mask, can be a single color or a color palette\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n    ):\n        self.color: Union[Color, ColorPalette] = color\n\n    def annotate(\n        self, scene: np.ndarray, detections: Detections, opacity: float = 0.5\n    ) -&gt; np.ndarray:\n\"\"\"\n        Overlays the masks on the given image based on the provided detections, with a specified opacity.\n\n        Parameters:\n            scene (np.ndarray): The image on which the masks will be overlaid\n            detections (Detections): The detections for which the masks will be overlaid\n            opacity (float): The opacity of the masks, between 0 and 1, default is 0.5\n\n        Returns:\n            np.ndarray: The image with the masks overlaid\n        \"\"\"\n        for i in range(len(detections.xyxy)):\n            if detections.mask is None:\n                continue\n\n            class_id = (\n                detections.class_id[i] if detections.class_id is not None else None\n            )\n            idx = class_id if class_id is not None else i\n            color = (\n                self.color.by_idx(idx)\n                if isinstance(self.color, ColorPalette)\n                else self.color\n            )\n\n            mask = detections.mask[i]\n            colored_mask = np.zeros_like(scene, dtype=np.uint8)\n            colored_mask[:] = color.as_bgr()\n\n            scene = np.where(\n                np.expand_dims(mask, axis=-1),\n                np.uint8(opacity * colored_mask + (1 - opacity) * scene),\n                scene,\n            )\n\n        return scene\n</code></pre>"},{"location":"detection/annotate/#supervision.detection.annotate.MaskAnnotator.annotate","title":"<code>annotate(scene, detections, opacity=0.5)</code>","text":"<p>Overlays the masks on the given image based on the provided detections, with a specified opacity.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The image on which the masks will be overlaid</p> required <code>detections</code> <code>Detections</code> <p>The detections for which the masks will be overlaid</p> required <code>opacity</code> <code>float</code> <p>The opacity of the masks, between 0 and 1, default is 0.5</p> <code>0.5</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The image with the masks overlaid</p> Source code in <code>supervision/detection/annotate.py</code> <pre><code>def annotate(\n    self, scene: np.ndarray, detections: Detections, opacity: float = 0.5\n) -&gt; np.ndarray:\n\"\"\"\n    Overlays the masks on the given image based on the provided detections, with a specified opacity.\n\n    Parameters:\n        scene (np.ndarray): The image on which the masks will be overlaid\n        detections (Detections): The detections for which the masks will be overlaid\n        opacity (float): The opacity of the masks, between 0 and 1, default is 0.5\n\n    Returns:\n        np.ndarray: The image with the masks overlaid\n    \"\"\"\n    for i in range(len(detections.xyxy)):\n        if detections.mask is None:\n            continue\n\n        class_id = (\n            detections.class_id[i] if detections.class_id is not None else None\n        )\n        idx = class_id if class_id is not None else i\n        color = (\n            self.color.by_idx(idx)\n            if isinstance(self.color, ColorPalette)\n            else self.color\n        )\n\n        mask = detections.mask[i]\n        colored_mask = np.zeros_like(scene, dtype=np.uint8)\n        colored_mask[:] = color.as_bgr()\n\n        scene = np.where(\n            np.expand_dims(mask, axis=-1),\n            np.uint8(opacity * colored_mask + (1 - opacity) * scene),\n            scene,\n        )\n\n    return scene\n</code></pre>"},{"location":"detection/core/","title":"Core","text":""},{"location":"detection/core/#detections","title":"Detections","text":"<p>Data class containing information about the detections in a video frame.</p> <p>Attributes:</p> Name Type Description <code>xyxy</code> <code>np.ndarray</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> <code>mask</code> <code>np.Optional[np.ndarray]</code> <p>(Optional[np.ndarray]): An array of shape <code>(n, W, H)</code> containing the segmentation masks.</p> <code>class_id</code> <code>Optional[np.ndarray]</code> <p>An array of shape <code>(n,)</code> containing the class ids of the detections.</p> <code>confidence</code> <code>Optional[np.ndarray]</code> <p>An array of shape <code>(n,)</code> containing the confidence scores of the detections.</p> <code>tracker_id</code> <code>Optional[np.ndarray]</code> <p>An array of shape <code>(n,)</code> containing the tracker ids of the detections.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>@dataclass\nclass Detections:\n\"\"\"\n    Data class containing information about the detections in a video frame.\n\n    Attributes:\n        xyxy (np.ndarray): An array of shape `(n, 4)` containing the bounding boxes coordinates in format `[x1, y1, x2, y2]`\n        mask: (Optional[np.ndarray]): An array of shape `(n, W, H)` containing the segmentation masks.\n        class_id (Optional[np.ndarray]): An array of shape `(n,)` containing the class ids of the detections.\n        confidence (Optional[np.ndarray]): An array of shape `(n,)` containing the confidence scores of the detections.\n        tracker_id (Optional[np.ndarray]): An array of shape `(n,)` containing the tracker ids of the detections.\n    \"\"\"\n\n    xyxy: np.ndarray\n    mask: np.Optional[np.ndarray] = None\n    class_id: Optional[np.ndarray] = None\n    confidence: Optional[np.ndarray] = None\n    tracker_id: Optional[np.ndarray] = None\n\n    def __post_init__(self):\n        n = len(self.xyxy)\n        _validate_xyxy(xyxy=self.xyxy, n=n)\n        _validate_mask(mask=self.mask, n=n)\n        _validate_class_id(class_id=self.class_id, n=n)\n        _validate_confidence(confidence=self.confidence, n=n)\n        _validate_tracker_id(tracker_id=self.tracker_id, n=n)\n\n    def __len__(self):\n\"\"\"\n        Returns the number of detections in the Detections object.\n        \"\"\"\n        return len(self.xyxy)\n\n    def __iter__(\n        self,\n    ) -&gt; Iterator[\n        Tuple[\n            np.ndarray,\n            Optional[np.ndarray],\n            Optional[float],\n            Optional[int],\n            Optional[int],\n        ]\n    ]:\n\"\"\"\n        Iterates over the Detections object and yield a tuple of `(xyxy, mask, confidence, class_id, tracker_id)` for each detection.\n        \"\"\"\n        for i in range(len(self.xyxy)):\n            yield (\n                self.xyxy[i],\n                self.mask[i] if self.mask is not None else None,\n                self.confidence[i] if self.confidence is not None else None,\n                self.class_id[i] if self.class_id is not None else None,\n                self.tracker_id[i] if self.tracker_id is not None else None,\n            )\n\n    def __eq__(self, other: Detections):\n        return all(\n            [\n                np.array_equal(self.xyxy, other.xyxy),\n                any(\n                    [\n                        self.mask is None and other.mask is None,\n                        np.array_equal(self.mask, other.mask),\n                    ]\n                ),\n                any(\n                    [\n                        self.class_id is None and other.class_id is None,\n                        np.array_equal(self.class_id, other.class_id),\n                    ]\n                ),\n                any(\n                    [\n                        self.confidence is None and other.confidence is None,\n                        np.array_equal(self.confidence, other.confidence),\n                    ]\n                ),\n                any(\n                    [\n                        self.tracker_id is None and other.tracker_id is None,\n                        np.array_equal(self.tracker_id, other.tracker_id),\n                    ]\n                ),\n            ]\n        )\n\n    @classmethod\n    def from_yolov5(cls, yolov5_results) -&gt; Detections:\n\"\"\"\n        Creates a Detections instance from a YOLOv5 output Detections\n\n        Args:\n            yolov5_results (yolov5.models.common.Detections): The output Detections instance from YOLOv5\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; from supervision import Detections\n\n            &gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n            &gt;&gt;&gt; results = model(IMAGE)\n            &gt;&gt;&gt; detections = Detections.from_yolov5(results)\n            ```\n        \"\"\"\n        yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n        return cls(\n            xyxy=yolov5_detections_predictions[:, :4],\n            confidence=yolov5_detections_predictions[:, 4],\n            class_id=yolov5_detections_predictions[:, 5].astype(int),\n        )\n\n    @classmethod\n    def from_yolov8(cls, yolov8_results) -&gt; Detections:\n\"\"\"\n        Creates a Detections instance from a YOLOv8 output Results\n\n        Args:\n            yolov8_results (ultralytics.yolo.engine.results.Results): The output Results instance from YOLOv8\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from ultralytics import YOLO\n            &gt;&gt;&gt; from supervision import Detections\n\n            &gt;&gt;&gt; model = YOLO('yolov8s.pt')\n            &gt;&gt;&gt; yolov8_results = model(IMAGE)[0]\n            &gt;&gt;&gt; detections = Detections.from_yolov8(yolov8_results)\n            ```\n        \"\"\"\n        return cls(\n            xyxy=yolov8_results.boxes.xyxy.cpu().numpy(),\n            confidence=yolov8_results.boxes.conf.cpu().numpy(),\n            class_id=yolov8_results.boxes.cls.cpu().numpy().astype(int),\n        )\n\n    @classmethod\n    def from_transformers(cls, transformers_results: dict) -&gt; Detections:\n\"\"\"\n        Creates a Detections instance from Object Detection Transformer output Results\n\n        Returns:\n            Detections: A new Detections object.\n        \"\"\"\n        return cls(\n            xyxy=transformers_results[\"boxes\"].cpu().numpy(),\n            confidence=transformers_results[\"scores\"].cpu().numpy(),\n            class_id=transformers_results[\"labels\"].cpu().numpy().astype(int),\n        )\n\n    @classmethod\n    def from_detectron2(cls, detectron2_results) -&gt; Detections:\n        return cls(\n            xyxy=detectron2_results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n            confidence=detectron2_results[\"instances\"].scores.cpu().numpy(),\n            class_id=detectron2_results[\"instances\"]\n            .pred_classes.cpu()\n            .numpy()\n            .astype(int),\n        )\n\n    @classmethod\n    def from_roboflow(cls, roboflow_result: dict, class_list: List[str]) -&gt; Detections:\n        xyxy = []\n        confidence = []\n        class_id = []\n\n        for prediction in roboflow_result[\"predictions\"]:\n            x = prediction[\"x\"]\n            y = prediction[\"y\"]\n            width = prediction[\"width\"]\n            height = prediction[\"height\"]\n            x_min = x - width / 2\n            y_min = y - height / 2\n            x_max = x_min + width\n            y_max = y_min + height\n            xyxy.append([x_min, y_min, x_max, y_max])\n            class_id.append(class_list.index(prediction[\"class\"]))\n            confidence.append(prediction[\"confidence\"])\n\n        return Detections(\n            xyxy=np.array(xyxy),\n            confidence=np.array(confidence),\n            class_id=np.array(class_id).astype(int),\n        )\n\n    @classmethod\n    def from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n\"\"\"\n        Creates a Detections instance from Segment Anything Model (SAM) by Meta AI.\n\n        Args:\n            sam_result (List[dict]): The output Results instance from SAM\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n            &gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n            &gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n            &gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n            ```\n        \"\"\"\n        sorted_generated_masks = sorted(\n            sam_result, key=lambda x: x[\"area\"], reverse=True\n        )\n\n        xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n        mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n        return Detections(xyxy=xywh_to_xyxy(boxes_xywh=xywh), mask=mask)\n\n    @classmethod\n    def from_coco_annotations(cls, coco_annotation: dict) -&gt; Detections:\n        xyxy, class_id = [], []\n\n        for annotation in coco_annotation:\n            x_min, y_min, width, height = annotation[\"bbox\"]\n            xyxy.append([x_min, y_min, x_min + width, y_min + height])\n            class_id.append(annotation[\"category_id\"])\n\n        return cls(xyxy=np.array(xyxy), class_id=np.array(class_id))\n\n    @classmethod\n    def empty(cls) -&gt; Detections:\n        return cls(\n            xyxy=np.empty((0, 4), dtype=np.float32),\n            confidence=np.array([], dtype=np.float32),\n            class_id=np.array([], dtype=int),\n        )\n\n    def get_anchor_coordinates(self, anchor: Position) -&gt; np.ndarray:\n\"\"\"\n        Returns the bounding box coordinates for a specific anchor.\n\n        Args:\n            anchor (Position): Position of bounding box anchor for which to return the coordinates.\n\n        Returns:\n            np.ndarray: An array of shape `(n, 2)` containing the bounding box anchor coordinates in format `[x, y]`.\n        \"\"\"\n        if anchor == Position.CENTER:\n            return np.array(\n                [\n                    (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.BOTTOM_CENTER:\n            return np.array(\n                [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n            ).transpose()\n\n        raise ValueError(f\"{anchor} is not supported.\")\n\n    def __getitem__(self, index: np.ndarray) -&gt; Detections:\n        if isinstance(index, np.ndarray) and (\n            index.dtype == bool or index.dtype == int\n        ):\n            return Detections(\n                xyxy=self.xyxy[index],\n                mask=self.mask[index] if self.mask is not None else None,\n                confidence=self.confidence[index]\n                if self.confidence is not None\n                else None,\n                class_id=self.class_id[index] if self.class_id is not None else None,\n                tracker_id=self.tracker_id[index]\n                if self.tracker_id is not None\n                else None,\n            )\n        raise TypeError(\n            f\"Detections.__getitem__ not supported for index of type {type(index)}.\"\n        )\n\n    @property\n    def area(self) -&gt; np.ndarray:\n\"\"\"\n        Calculate the area of each detection in the set of object detections. If masks field is defined property\n        returns are of each mask. If only box is given property return area of each box.\n\n        Returns:\n          np.ndarray: An array of floats containing the area of each detection in the format of `(area_1, area_2, ..., area_n)`, where n is the number of detections.\n        \"\"\"\n        if self.mask is not None:\n            return np.array([np.sum(mask) for mask in self.mask])\n        else:\n            return self.box_area\n\n    @property\n    def box_area(self) -&gt; np.ndarray:\n\"\"\"\n        Calculate the area of each bounding box in the set of object detections.\n\n        Returns:\n            np.ndarray: An array of floats containing the area of each bounding box in the format of `(area_1, area_2, ..., area_n)`, where n is the number of detections.\n        \"\"\"\n        return (self.xyxy[:, 3] - self.xyxy[:, 1]) * (self.xyxy[:, 2] - self.xyxy[:, 0])\n\n    def with_nms(\n        self, threshold: float = 0.5, class_agnostic: bool = False\n    ) -&gt; Detections:\n\"\"\"\n        Perform non-maximum suppression on the current set of object detections.\n\n        Args:\n            threshold (float, optional): The intersection-over-union threshold to use for non-maximum suppression. Defaults to 0.5.\n            class_agnostic (bool, optional): Whether to perform class-agnostic non-maximum suppression. If True, the class_id of each detection will be ignored. Defaults to False.\n\n        Returns:\n            Detections: A new Detections object containing the subset of detections after non-maximum suppression.\n\n        Raises:\n            AssertionError: If `confidence` is None and class_agnostic is False. If `class_id` is None and class_agnostic is False.\n        \"\"\"\n        if len(self) == 0:\n            return self\n\n        assert (\n            self.confidence is not None\n        ), f\"Detections confidence must be given for NMS to be executed.\"\n\n        if class_agnostic:\n            predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n            indices = non_max_suppression(\n                predictions=predictions, iou_threshold=threshold\n            )\n            return self[indices]\n\n        assert self.class_id is not None, (\n            f\"Detections class_id must be given for NMS to be executed. If you intended to perform class agnostic \"\n            f\"NMS set class_agnostic=True.\"\n        )\n\n        predictions = np.hstack(\n            (self.xyxy, self.confidence.reshape(-1, 1), self.class_id.reshape(-1, 1))\n        )\n        indices = non_max_suppression(predictions=predictions, iou_threshold=threshold)\n        return self[indices]\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.area","title":"<code>area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each detection in the set of object detections. If masks field is defined property returns are of each mask. If only box is given property return area of each box.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: An array of floats containing the area of each detection in the format of <code>(area_1, area_2, ..., area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections.box_area","title":"<code>box_area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each bounding box in the set of object detections.</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: An array of floats containing the area of each bounding box in the format of <code>(area_1, area_2, ..., area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the Detections object and yield a tuple of <code>(xyxy, mask, confidence, class_id, tracker_id)</code> for each detection.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[\n    Tuple[\n        np.ndarray,\n        Optional[np.ndarray],\n        Optional[float],\n        Optional[int],\n        Optional[int],\n    ]\n]:\n\"\"\"\n    Iterates over the Detections object and yield a tuple of `(xyxy, mask, confidence, class_id, tracker_id)` for each detection.\n    \"\"\"\n    for i in range(len(self.xyxy)):\n        yield (\n            self.xyxy[i],\n            self.mask[i] if self.mask is not None else None,\n            self.confidence[i] if self.confidence is not None else None,\n            self.class_id[i] if self.class_id is not None else None,\n            self.tracker_id[i] if self.tracker_id is not None else None,\n        )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of detections in the Detections object.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __len__(self):\n\"\"\"\n    Returns the number of detections in the Detections object.\n    \"\"\"\n    return len(self.xyxy)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_sam","title":"<code>from_sam(sam_result)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Segment Anything Model (SAM) by Meta AI.</p> <p>Parameters:</p> Name Type Description Default <code>sam_result</code> <code>List[dict]</code> <p>The output Results instance from SAM</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n&gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n&gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n&gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n\"\"\"\n    Creates a Detections instance from Segment Anything Model (SAM) by Meta AI.\n\n    Args:\n        sam_result (List[dict]): The output Results instance from SAM\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n        &gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n        &gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n        &gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n        ```\n    \"\"\"\n    sorted_generated_masks = sorted(\n        sam_result, key=lambda x: x[\"area\"], reverse=True\n    )\n\n    xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n    mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n    return Detections(xyxy=xywh_to_xyxy(boxes_xywh=xywh), mask=mask)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_transformers","title":"<code>from_transformers(transformers_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Object Detection Transformer output Results</p> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_transformers(cls, transformers_results: dict) -&gt; Detections:\n\"\"\"\n    Creates a Detections instance from Object Detection Transformer output Results\n\n    Returns:\n        Detections: A new Detections object.\n    \"\"\"\n    return cls(\n        xyxy=transformers_results[\"boxes\"].cpu().numpy(),\n        confidence=transformers_results[\"scores\"].cpu().numpy(),\n        class_id=transformers_results[\"labels\"].cpu().numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolov5","title":"<code>from_yolov5(yolov5_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLOv5 output Detections</p> <p>Parameters:</p> Name Type Description Default <code>yolov5_results</code> <code>yolov5.models.common.Detections</code> <p>The output Detections instance from YOLOv5</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from supervision import Detections\n\n&gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n&gt;&gt;&gt; results = model(IMAGE)\n&gt;&gt;&gt; detections = Detections.from_yolov5(results)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolov5(cls, yolov5_results) -&gt; Detections:\n\"\"\"\n    Creates a Detections instance from a YOLOv5 output Detections\n\n    Args:\n        yolov5_results (yolov5.models.common.Detections): The output Detections instance from YOLOv5\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; from supervision import Detections\n\n        &gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n        &gt;&gt;&gt; results = model(IMAGE)\n        &gt;&gt;&gt; detections = Detections.from_yolov5(results)\n        ```\n    \"\"\"\n    yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n    return cls(\n        xyxy=yolov5_detections_predictions[:, :4],\n        confidence=yolov5_detections_predictions[:, 4],\n        class_id=yolov5_detections_predictions[:, 5].astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolov8","title":"<code>from_yolov8(yolov8_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLOv8 output Results</p> <p>Parameters:</p> Name Type Description Default <code>yolov8_results</code> <code>ultralytics.yolo.engine.results.Results</code> <p>The output Results instance from YOLOv8</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; from ultralytics import YOLO\n&gt;&gt;&gt; from supervision import Detections\n\n&gt;&gt;&gt; model = YOLO('yolov8s.pt')\n&gt;&gt;&gt; yolov8_results = model(IMAGE)[0]\n&gt;&gt;&gt; detections = Detections.from_yolov8(yolov8_results)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolov8(cls, yolov8_results) -&gt; Detections:\n\"\"\"\n    Creates a Detections instance from a YOLOv8 output Results\n\n    Args:\n        yolov8_results (ultralytics.yolo.engine.results.Results): The output Results instance from YOLOv8\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from ultralytics import YOLO\n        &gt;&gt;&gt; from supervision import Detections\n\n        &gt;&gt;&gt; model = YOLO('yolov8s.pt')\n        &gt;&gt;&gt; yolov8_results = model(IMAGE)[0]\n        &gt;&gt;&gt; detections = Detections.from_yolov8(yolov8_results)\n        ```\n    \"\"\"\n    return cls(\n        xyxy=yolov8_results.boxes.xyxy.cpu().numpy(),\n        confidence=yolov8_results.boxes.conf.cpu().numpy(),\n        class_id=yolov8_results.boxes.cls.cpu().numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.get_anchor_coordinates","title":"<code>get_anchor_coordinates(anchor)</code>","text":"<p>Returns the bounding box coordinates for a specific anchor.</p> <p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>Position</code> <p>Position of bounding box anchor for which to return the coordinates.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: An array of shape <code>(n, 2)</code> containing the bounding box anchor coordinates in format <code>[x, y]</code>.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def get_anchor_coordinates(self, anchor: Position) -&gt; np.ndarray:\n\"\"\"\n    Returns the bounding box coordinates for a specific anchor.\n\n    Args:\n        anchor (Position): Position of bounding box anchor for which to return the coordinates.\n\n    Returns:\n        np.ndarray: An array of shape `(n, 2)` containing the bounding box anchor coordinates in format `[x, y]`.\n    \"\"\"\n    if anchor == Position.CENTER:\n        return np.array(\n            [\n                (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.BOTTOM_CENTER:\n        return np.array(\n            [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n        ).transpose()\n\n    raise ValueError(f\"{anchor} is not supported.\")\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.with_nms","title":"<code>with_nms(threshold=0.5, class_agnostic=False)</code>","text":"<p>Perform non-maximum suppression on the current set of object detections.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression. Defaults to 0.5.</p> <code>0.5</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to perform class-agnostic non-maximum suppression. If True, the class_id of each detection will be ignored. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object containing the subset of detections after non-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>confidence</code> is None and class_agnostic is False. If <code>class_id</code> is None and class_agnostic is False.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def with_nms(\n    self, threshold: float = 0.5, class_agnostic: bool = False\n) -&gt; Detections:\n\"\"\"\n    Perform non-maximum suppression on the current set of object detections.\n\n    Args:\n        threshold (float, optional): The intersection-over-union threshold to use for non-maximum suppression. Defaults to 0.5.\n        class_agnostic (bool, optional): Whether to perform class-agnostic non-maximum suppression. If True, the class_id of each detection will be ignored. Defaults to False.\n\n    Returns:\n        Detections: A new Detections object containing the subset of detections after non-maximum suppression.\n\n    Raises:\n        AssertionError: If `confidence` is None and class_agnostic is False. If `class_id` is None and class_agnostic is False.\n    \"\"\"\n    if len(self) == 0:\n        return self\n\n    assert (\n        self.confidence is not None\n    ), f\"Detections confidence must be given for NMS to be executed.\"\n\n    if class_agnostic:\n        predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n        indices = non_max_suppression(\n            predictions=predictions, iou_threshold=threshold\n        )\n        return self[indices]\n\n    assert self.class_id is not None, (\n        f\"Detections class_id must be given for NMS to be executed. If you intended to perform class agnostic \"\n        f\"NMS set class_agnostic=True.\"\n    )\n\n    predictions = np.hstack(\n        (self.xyxy, self.confidence.reshape(-1, 1), self.class_id.reshape(-1, 1))\n    )\n    indices = non_max_suppression(predictions=predictions, iou_threshold=threshold)\n    return self[indices]\n</code></pre>"},{"location":"detection/utils/","title":"Utils","text":""},{"location":"detection/utils/#generate_2d_mask","title":"generate_2d_mask","text":"<p>Generate a 2D mask from a polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>np.ndarray</code> <p>The polygon for which the mask should be generated, given as a list of vertices.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The width and height of the desired resolution.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The generated 2D mask, where the polygon is marked with <code>1</code>'s and the rest is filled with <code>0</code>'s.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def generate_2d_mask(polygon: np.ndarray, resolution_wh: Tuple[int, int]) -&gt; np.ndarray:\n\"\"\"Generate a 2D mask from a polygon.\n\n    Args:\n        polygon (np.ndarray): The polygon for which the mask should be generated, given as a list of vertices.\n        resolution_wh (Tuple[int, int]): The width and height of the desired resolution.\n\n    Returns:\n        np.ndarray: The generated 2D mask, where the polygon is marked with `1`'s and the rest is filled with `0`'s.\n    \"\"\"\n    width, height = resolution_wh\n    mask = np.zeros((height, width), dtype=np.uint8)\n    cv2.fillPoly(mask, [polygon], color=1)\n    return mask\n</code></pre>"},{"location":"detection/utils/#box_iou_batch","title":"box_iou_batch","text":"<p>Compute Intersection over Union (IoU) of two sets of bounding boxes - <code>boxes_true</code> and <code>boxes_detection</code>. Both sets of boxes are expected to be in <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_true</code> <code>np.ndarray</code> <p>2D <code>np.ndarray</code> representing ground-truth boxes. <code>shape = (N, 4)</code> where <code>N</code> is number of true objects.</p> required <code>boxes_detection</code> <code>np.ndarray</code> <p>2D <code>np.ndarray</code> representing detection boxes. <code>shape = (M, 4)</code> where <code>M</code> is number of detected objects.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Pairwise IoU of boxes from <code>boxes_true</code> and <code>boxes_detection</code>. <code>shape = (N, M)</code> where <code>N</code> is number of true objects and <code>M</code> is number of detected objects.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def box_iou_batch(boxes_true: np.ndarray, boxes_detection: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Compute Intersection over Union (IoU) of two sets of bounding boxes - `boxes_true` and `boxes_detection`. Both sets\n    of boxes are expected to be in `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        boxes_true (np.ndarray): 2D `np.ndarray` representing ground-truth boxes. `shape = (N, 4)` where `N` is number of true objects.\n        boxes_detection (np.ndarray): 2D `np.ndarray` representing detection boxes. `shape = (M, 4)` where `M` is number of detected objects.\n\n    Returns:\n        np.ndarray: Pairwise IoU of boxes from `boxes_true` and `boxes_detection`. `shape = (N, M)` where `N` is number of true objects and `M` is number of detected objects.\n    \"\"\"\n\n    def box_area(box):\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area_true = box_area(boxes_true.T)\n    area_detection = box_area(boxes_detection.T)\n\n    top_left = np.maximum(boxes_true[:, None, :2], boxes_detection[:, :2])\n    bottom_right = np.minimum(boxes_true[:, None, 2:], boxes_detection[:, 2:])\n\n    area_inter = np.prod(np.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n    return area_inter / (area_true[:, None] + area_detection - area_inter)\n</code></pre>"},{"location":"detection/utils/#non_max_suppression","title":"non_max_suppression","text":"<p>Perform Non-Maximum Suppression (NMS) on object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>An array of object detection predictions in the format of <code>(x_min, y_min, x_max, y_max, score)</code> or <code>(x_min, y_min, x_max, y_max, score, class)</code>.</p> required <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: A boolean array indicating which predictions to keep after non-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>iou_threshold</code> is not within the closed range from <code>0</code> to <code>1</code>.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def non_max_suppression(\n    predictions: np.ndarray, iou_threshold: float = 0.5\n) -&gt; np.ndarray:\n\"\"\"\n    Perform Non-Maximum Suppression (NMS) on object detection predictions.\n\n    Args:\n        predictions (np.ndarray): An array of object detection predictions in the format of `(x_min, y_min, x_max, y_max, score)` or `(x_min, y_min, x_max, y_max, score, class)`.\n        iou_threshold (float, optional): The intersection-over-union threshold to use for non-maximum suppression.\n\n    Returns:\n        np.ndarray: A boolean array indicating which predictions to keep after non-maximum suppression.\n\n    Raises:\n        AssertionError: If `iou_threshold` is not within the closed range from `0` to `1`.\n    \"\"\"\n    assert 0 &lt;= iou_threshold &lt;= 1, (\n        f\"Value of `iou_threshold` must be in the closed range from 0 to 1, \"\n        f\"{iou_threshold} given.\"\n    )\n    rows, columns = predictions.shape\n\n    # add column #5 - category filled with zeros for agnostic nms\n    if columns == 5:\n        predictions = np.c_[predictions, np.zeros(rows)]\n\n    # sort predictions column #4 - score\n    sort_index = np.flip(predictions[:, 4].argsort())\n    predictions = predictions[sort_index]\n\n    boxes = predictions[:, :4]\n    categories = predictions[:, 5]\n    ious = box_iou_batch(boxes, boxes)\n    ious = ious - np.eye(rows)\n\n    keep = np.ones(rows, dtype=bool)\n\n    for index, (iou, category) in enumerate(zip(ious, categories)):\n        if not keep[index]:\n            continue\n\n        # drop detections with iou &gt; iou_threshold and same category as current detections\n        condition = (iou &gt; iou_threshold) &amp; (categories == category)\n        keep = keep &amp; ~condition\n\n    return keep[sort_index.argsort()]\n</code></pre>"},{"location":"detection/utils/#mask_to_xyxy","title":"mask_to_xyxy","text":"<p>Converts a 3D <code>np.array</code> of 2D bool masks into a 2D <code>np.array</code> of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>np.ndarray</code> <p>A 3D <code>np.array</code> of shape <code>(N, W, H)</code> containing 2D bool masks</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: A 2D <code>np.array</code> of shape <code>(N, 4)</code> containing the bounding boxes <code>(x_min, y_min, x_max, y_max)</code> for each mask</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_to_xyxy(masks: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Converts a 3D `np.array` of 2D bool masks into a 2D `np.array` of bounding boxes.\n\n    Parameters:\n        masks (np.ndarray): A 3D `np.array` of shape `(N, W, H)` containing 2D bool masks\n\n    Returns:\n        np.ndarray: A 2D `np.array` of shape `(N, 4)` containing the bounding boxes `(x_min, y_min, x_max, y_max)` for each mask\n    \"\"\"\n    n = masks.shape[0]\n    bboxes = np.zeros((n, 4), dtype=int)\n\n    for i, mask in enumerate(masks):\n        rows, cols = np.where(mask)\n\n        if len(rows) &gt; 0 and len(cols) &gt; 0:\n            x_min, x_max = np.min(cols), np.max(cols)\n            y_min, y_max = np.min(rows), np.max(rows)\n            bboxes[i, :] = [x_min, y_min, x_max, y_max]\n\n    return bboxes\n</code></pre>"},{"location":"detection/tools/polygon_zone/","title":"Polygon Zone","text":""},{"location":"detection/tools/polygon_zone/#polygonzone","title":"PolygonZone","text":"<p>A class for defining a polygon-shaped zone within a frame for detecting objects.</p> <p>Attributes:</p> Name Type Description <code>polygon</code> <code>np.ndarray</code> <p>A numpy array defining the polygon vertices</p> <code>frame_resolution_wh</code> <code>Tuple[int, int]</code> <p>The frame resolution (width, height)</p> <code>triggering_position</code> <code>Position</code> <p>The position within the bounding box that triggers the zone (default: Position.BOTTOM_CENTER)</p> <code>current_count</code> <code>int</code> <p>The current count of detected objects within the zone</p> <code>mask</code> <code>np.ndarray</code> <p>The 2D bool mask for the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZone:\n\"\"\"\n    A class for defining a polygon-shaped zone within a frame for detecting objects.\n\n    Attributes:\n        polygon (np.ndarray): A numpy array defining the polygon vertices\n        frame_resolution_wh (Tuple[int, int]): The frame resolution (width, height)\n        triggering_position (Position): The position within the bounding box that triggers the zone (default: Position.BOTTOM_CENTER)\n        current_count (int): The current count of detected objects within the zone\n        mask (np.ndarray): The 2D bool mask for the polygon zone\n    \"\"\"\n\n    def __init__(\n        self,\n        polygon: np.ndarray,\n        frame_resolution_wh: Tuple[int, int],\n        triggering_position: Position = Position.BOTTOM_CENTER,\n    ):\n        self.polygon = polygon\n        self.frame_resolution_wh = frame_resolution_wh\n        self.triggering_position = triggering_position\n        self.current_count = 0\n\n        width, height = frame_resolution_wh\n        self.mask = generate_2d_mask(\n            polygon=polygon, resolution_wh=(width + 1, height + 1)\n        )\n\n    def trigger(self, detections: Detections) -&gt; np.ndarray:\n\"\"\"\n        Determines if the detections are within the polygon zone.\n\n        Parameters:\n            detections (Detections): The detections to be checked against the polygon zone\n\n        Returns:\n            np.ndarray: A boolean numpy array indicating if each detection is within the polygon zone\n        \"\"\"\n\n        clipped_xyxy = clip_boxes(\n            boxes_xyxy=detections.xyxy, frame_resolution_wh=self.frame_resolution_wh\n        )\n        clipped_detections = replace(detections, xyxy=clipped_xyxy)\n        clipped_anchors = np.ceil(\n            clipped_detections.get_anchor_coordinates(anchor=self.triggering_position)\n        ).astype(int)\n        is_in_zone = self.mask[clipped_anchors[:, 1], clipped_anchors[:, 0]]\n        self.current_count = np.sum(is_in_zone)\n        return is_in_zone.astype(bool)\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZone.trigger","title":"<code>trigger(detections)</code>","text":"<p>Determines if the detections are within the polygon zone.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to be checked against the polygon zone</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: A boolean numpy array indicating if each detection is within the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def trigger(self, detections: Detections) -&gt; np.ndarray:\n\"\"\"\n    Determines if the detections are within the polygon zone.\n\n    Parameters:\n        detections (Detections): The detections to be checked against the polygon zone\n\n    Returns:\n        np.ndarray: A boolean numpy array indicating if each detection is within the polygon zone\n    \"\"\"\n\n    clipped_xyxy = clip_boxes(\n        boxes_xyxy=detections.xyxy, frame_resolution_wh=self.frame_resolution_wh\n    )\n    clipped_detections = replace(detections, xyxy=clipped_xyxy)\n    clipped_anchors = np.ceil(\n        clipped_detections.get_anchor_coordinates(anchor=self.triggering_position)\n    ).astype(int)\n    is_in_zone = self.mask[clipped_anchors[:, 1], clipped_anchors[:, 0]]\n    self.current_count = np.sum(is_in_zone)\n    return is_in_zone.astype(bool)\n</code></pre>"},{"location":"detection/tools/polygon_zone/#polygonzoneannotator","title":"PolygonZoneAnnotator","text":"<p>A class for annotating a polygon-shaped zone within a frame with a count of detected objects.</p> <p>Attributes:</p> Name Type Description <code>zone</code> <code>PolygonZone</code> <p>The polygon zone to be annotated</p> <code>color</code> <code>Color</code> <p>The color to draw the polygon lines</p> <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, default is 2</p> <code>text_color</code> <code>Color</code> <p>The color of the text on the polygon, default is black</p> <code>text_scale</code> <code>float</code> <p>The scale of the text on the polygon, default is 0.5</p> <code>text_thickness</code> <code>int</code> <p>The thickness of the text on the polygon, default is 1</p> <code>text_padding</code> <code>int</code> <p>The padding around the text on the polygon, default is 10</p> <code>font</code> <code>int</code> <p>The font type for the text on the polygon, default is cv2.FONT_HERSHEY_SIMPLEX</p> <code>center</code> <code>Tuple[int, int]</code> <p>The center of the polygon for text placement</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZoneAnnotator:\n\"\"\"\n    A class for annotating a polygon-shaped zone within a frame with a count of detected objects.\n\n    Attributes:\n        zone (PolygonZone): The polygon zone to be annotated\n        color (Color): The color to draw the polygon lines\n        thickness (int): The thickness of the polygon lines, default is 2\n        text_color (Color): The color of the text on the polygon, default is black\n        text_scale (float): The scale of the text on the polygon, default is 0.5\n        text_thickness (int): The thickness of the text on the polygon, default is 1\n        text_padding (int): The padding around the text on the polygon, default is 10\n        font (int): The font type for the text on the polygon, default is cv2.FONT_HERSHEY_SIMPLEX\n        center (Tuple[int, int]): The center of the polygon for text placement\n    \"\"\"\n\n    def __init__(\n        self,\n        zone: PolygonZone,\n        color: Color,\n        thickness: int = 2,\n        text_color: Color = Color.black(),\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n    ):\n        self.zone = zone\n        self.color = color\n        self.thickness = thickness\n        self.text_color = text_color\n        self.text_scale = text_scale\n        self.text_thickness = text_thickness\n        self.text_padding = text_padding\n        self.font = cv2.FONT_HERSHEY_SIMPLEX\n        self.center = get_polygon_center(polygon=zone.polygon)\n\n    def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n\"\"\"\n        Annotates the polygon zone within a frame with a count of detected objects.\n\n        Parameters:\n            scene (np.ndarray): The image on which the polygon zone will be annotated\n            label (Optional[str]): An optional label for the count of detected objects within the polygon zone (default: None)\n\n        Returns:\n            np.ndarray: The image with the polygon zone and count of detected objects\n        \"\"\"\n        annotated_frame = draw_polygon(\n            scene=scene,\n            polygon=self.zone.polygon,\n            color=self.color,\n            thickness=self.thickness,\n        )\n\n        annotated_frame = draw_text(\n            scene=annotated_frame,\n            text=str(self.zone.current_count) if label is None else label,\n            text_anchor=self.center,\n            background_color=self.color,\n            text_color=self.text_color,\n            text_scale=self.text_scale,\n            text_thickness=self.text_thickness,\n            text_padding=self.text_padding,\n            text_font=self.font,\n        )\n\n        return annotated_frame\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZoneAnnotator.annotate","title":"<code>annotate(scene, label=None)</code>","text":"<p>Annotates the polygon zone within a frame with a count of detected objects.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The image on which the polygon zone will be annotated</p> required <code>label</code> <code>Optional[str]</code> <p>An optional label for the count of detected objects within the polygon zone (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The image with the polygon zone and count of detected objects</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n\"\"\"\n    Annotates the polygon zone within a frame with a count of detected objects.\n\n    Parameters:\n        scene (np.ndarray): The image on which the polygon zone will be annotated\n        label (Optional[str]): An optional label for the count of detected objects within the polygon zone (default: None)\n\n    Returns:\n        np.ndarray: The image with the polygon zone and count of detected objects\n    \"\"\"\n    annotated_frame = draw_polygon(\n        scene=scene,\n        polygon=self.zone.polygon,\n        color=self.color,\n        thickness=self.thickness,\n    )\n\n    annotated_frame = draw_text(\n        scene=annotated_frame,\n        text=str(self.zone.current_count) if label is None else label,\n        text_anchor=self.center,\n        background_color=self.color,\n        text_color=self.text_color,\n        text_scale=self.text_scale,\n        text_thickness=self.text_thickness,\n        text_padding=self.text_padding,\n        text_font=self.font,\n    )\n\n    return annotated_frame\n</code></pre>"},{"location":"draw/utils/","title":"Utils","text":""},{"location":"draw/utils/#draw_line","title":"draw_line","text":"<p>Draws a line on a given scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The scene on which the line will be drawn</p> required <code>start</code> <code>Point</code> <p>The starting point of the line</p> required <code>end</code> <code>Point</code> <p>The end point of the line</p> required <code>color</code> <code>Color</code> <p>The color of the line</p> required <code>thickness</code> <code>int</code> <p>The thickness of the line</p> <code>2</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The scene with the line drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_line(\n    scene: np.ndarray, start: Point, end: Point, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n\"\"\"\n    Draws a line on a given scene.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the line will be drawn\n        start (Point): The starting point of the line\n        end (Point): The end point of the line\n        color (Color): The color of the line\n        thickness (int): The thickness of the line\n\n    Returns:\n        np.ndarray: The scene with the line drawn on it\n    \"\"\"\n    cv2.line(\n        scene,\n        start.as_xy_int_tuple(),\n        end.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_rectangle","title":"draw_rectangle","text":"<p>Draws a rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <code>thickness</code> <code>int</code> <p>The thickness of the rectangle border</p> <code>2</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_rectangle(\n    scene: np.ndarray, rect: Rect, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n\"\"\"\n    Draws a rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n        thickness (int): The thickness of the rectangle border\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_filled_rectangle","title":"draw_filled_rectangle","text":"<p>Draws a filled rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_filled_rectangle(scene: np.ndarray, rect: Rect, color: Color) -&gt; np.ndarray:\n\"\"\"\n    Draws a filled rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        -1,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_polygon","title":"draw_polygon","text":"<p>Draw a polygon on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>The scene to draw the polygon on.</p> required <code>polygon</code> <code>np.ndarray</code> <p>The polygon to be drawn, given as a list of vertices.</p> required <code>color</code> <code>Color</code> <p>The color of the polygon.</p> required <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, by default 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The scene with the polygon drawn on it.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_polygon(\n    scene: np.ndarray, polygon: np.ndarray, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n\"\"\"Draw a polygon on a scene.\n\n    Parameters:\n        scene (np.ndarray): The scene to draw the polygon on.\n        polygon (np.ndarray): The polygon to be drawn, given as a list of vertices.\n        color (Color): The color of the polygon.\n        thickness (int, optional): The thickness of the polygon lines, by default 2.\n\n    Returns:\n        np.ndarray: The scene with the polygon drawn on it.\n    \"\"\"\n    cv2.polylines(\n        scene, [polygon], isClosed=True, color=color.as_bgr(), thickness=thickness\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_text","title":"draw_text","text":"<p>Draw text with background on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>np.ndarray</code> <p>A 2-dimensional numpy ndarray representing an image or scene.</p> required <code>text</code> <code>str</code> <p>The text to be drawn.</p> required <code>text_anchor</code> <code>Point</code> <p>The anchor point for the text, represented as a Point object with x and y attributes.</p> required <code>text_color</code> <code>Color</code> <p>The color of the text. Defaults to black.</p> <code>Color.black()</code> <code>text_scale</code> <code>float</code> <p>The scale of the text. Defaults to 0.5.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>The thickness of the text. Defaults to 1.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>The amount of padding to add around the text when drawing a rectangle in the background. Defaults to 10.</p> <code>10</code> <code>text_font</code> <code>int</code> <p>The font to use for the text. Defaults to cv2.FONT_HERSHEY_SIMPLEX.</p> <code>cv2.FONT_HERSHEY_SIMPLEX</code> <code>background_color</code> <code>Color</code> <p>The color of the background rectangle, if one is to be drawn. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: The input scene with the text drawn on it.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scene = np.zeros((100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; text_anchor = Point(x=50, y=50)\n&gt;&gt;&gt; scene = draw_text(scene=scene, text=\"Hello, world!\", text_anchor=text_anchor)\n</code></pre> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_text(\n    scene: np.ndarray,\n    text: str,\n    text_anchor: Point,\n    text_color: Color = Color.black(),\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    text_font: int = cv2.FONT_HERSHEY_SIMPLEX,\n    background_color: Optional[Color] = None,\n) -&gt; np.ndarray:\n\"\"\"\n    Draw text with background on a scene.\n\n    Parameters:\n        scene (np.ndarray): A 2-dimensional numpy ndarray representing an image or scene.\n        text (str): The text to be drawn.\n        text_anchor (Point): The anchor point for the text, represented as a Point object with x and y attributes.\n        text_color (Color, optional): The color of the text. Defaults to black.\n        text_scale (float, optional): The scale of the text. Defaults to 0.5.\n        text_thickness (int, optional): The thickness of the text. Defaults to 1.\n        text_padding (int, optional): The amount of padding to add around the text when drawing a rectangle in the background. Defaults to 10.\n        text_font (int, optional): The font to use for the text. Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n        background_color (Color, optional): The color of the background rectangle, if one is to be drawn. Defaults to None.\n\n    Returns:\n        np.ndarray: The input scene with the text drawn on it.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; scene = np.zeros((100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; text_anchor = Point(x=50, y=50)\n        &gt;&gt;&gt; scene = draw_text(scene=scene, text=\"Hello, world!\", text_anchor=text_anchor)\n        ```\n    \"\"\"\n    text_width, text_height = cv2.getTextSize(\n        text=text,\n        fontFace=text_font,\n        fontScale=text_scale,\n        thickness=text_thickness,\n    )[0]\n    text_rect = Rect(\n        x=text_anchor.x - text_width // 2,\n        y=text_anchor.y - text_height // 2,\n        width=text_width,\n        height=text_height,\n    ).pad(text_padding)\n\n    if background_color is not None:\n        scene = draw_filled_rectangle(\n            scene=scene, rect=text_rect, color=background_color\n        )\n\n    cv2.putText(\n        img=scene,\n        text=text,\n        org=(text_anchor.x - text_width // 2, text_anchor.y + text_height // 2),\n        fontFace=text_font,\n        fontScale=text_scale,\n        color=text_color.as_bgr(),\n        thickness=text_thickness,\n        lineType=cv2.LINE_AA,\n    )\n    return scene\n</code></pre>"},{"location":"notebook/utils/","title":"Utils","text":""},{"location":"notebook/utils/#plot_image","title":"plot_image","text":"<p>Plots image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The frame to be displayed.</p> required <code>size</code> <code>Tuple[int, int]</code> <p>The size of the plot.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(\"path/to/image.jpg\")\n\n%matplotlib inline\n&gt;&gt;&gt; sv.plot_image(image, (16, 16))\n</code></pre> Source code in <code>supervision/notebook/utils.py</code> <pre><code>def plot_image(\n    image: np.ndarray, size: Tuple[int, int] = (12, 12), cmap: Optional[str] = \"gray\"\n) -&gt; None:\n\"\"\"\n    Plots image using matplotlib.\n\n    Args:\n        image (np.ndarray): The frame to be displayed.\n        size (Tuple[int, int]): The size of the plot.\n        cmap (str): the colormap to use for single channel images.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(\"path/to/image.jpg\")\n\n        %matplotlib inline\n        &gt;&gt;&gt; sv.plot_image(image, (16, 16))\n        ```\n    \"\"\"\n    plt.figure(figsize=size)\n\n    if image.ndim == 2:\n        plt.imshow(image, cmap=cmap)\n    else:\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"notebook/utils/#plot_images_grid","title":"plot_images_grid","text":"<p>Plots images in a grid using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[np.ndarray]</code> <p>A list of images as numpy arrays.</p> required <code>grid_size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the number of rows and columns for the grid.</p> required <code>titles</code> <code>Optional[List[str]]</code> <p>A list of titles for each image. Defaults to None.</p> <code>None</code> <code>size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the width and height of the entire plot in inches.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images exceeds the grid size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image1 = cv2.imread(\"path/to/image1.jpg\")\n&gt;&gt;&gt; image2 = cv2.imread(\"path/to/image2.jpg\")\n&gt;&gt;&gt; image3 = cv2.imread(\"path/to/image3.jpg\")\n\n&gt;&gt;&gt; images = [image1, image2, image3]\n&gt;&gt;&gt; titles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n%matplotlib inline\n&gt;&gt;&gt; plot_images_grid(images, grid_size=(2, 2), titles=titles, figsize=(16, 16))\n</code></pre> Source code in <code>supervision/notebook/utils.py</code> <pre><code>def plot_images_grid(\n    images: List[np.ndarray],\n    grid_size: Tuple[int, int],\n    titles: Optional[List[str]] = None,\n    size: Tuple[int, int] = (12, 12),\n    cmap: Optional[str] = \"gray\",\n) -&gt; None:\n\"\"\"\n    Plots images in a grid using matplotlib.\n\n    Args:\n       images (List[np.ndarray]): A list of images as numpy arrays.\n       grid_size (Tuple[int, int]): A tuple specifying the number of rows and columns for the grid.\n       titles (Optional[List[str]]): A list of titles for each image. Defaults to None.\n       size (Tuple[int, int]): A tuple specifying the width and height of the entire plot in inches.\n       cmap (str): the colormap to use for single channel images.\n\n    Raises:\n       ValueError: If the number of images exceeds the grid size.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image1 = cv2.imread(\"path/to/image1.jpg\")\n        &gt;&gt;&gt; image2 = cv2.imread(\"path/to/image2.jpg\")\n        &gt;&gt;&gt; image3 = cv2.imread(\"path/to/image3.jpg\")\n\n        &gt;&gt;&gt; images = [image1, image2, image3]\n        &gt;&gt;&gt; titles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n        %matplotlib inline\n        &gt;&gt;&gt; plot_images_grid(images, grid_size=(2, 2), titles=titles, figsize=(16, 16))\n        ```\n    \"\"\"\n    nrows, ncols = grid_size\n\n    if len(images) &gt; nrows * ncols:\n        raise ValueError(\n            \"The number of images exceeds the grid size. Please increase the grid size or reduce the number of images.\"\n        )\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=size)\n\n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(images):\n            if images[idx].ndim == 2:\n                ax.imshow(images[idx], cmap=cmap)\n            else:\n                ax.imshow(cv2.cvtColor(images[idx], cv2.COLOR_BGR2RGB))\n\n            if titles is not None and idx &lt; len(titles):\n                ax.set_title(titles[idx])\n\n        ax.axis(\"off\")\n    plt.show()\n</code></pre>"}]}